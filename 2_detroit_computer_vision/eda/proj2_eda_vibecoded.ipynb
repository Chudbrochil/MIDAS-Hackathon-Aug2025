{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Detroit Blight Classification - Exploratory Data Analysis\n",
    "\n",
    "This notebook analyzes three Detroit datasets to identify potential blight/inhabitability labels and features:\n",
    "1. **Blight Survey Data** - DLBA property condition assessments\n",
    "2. **COD Layers CSV** - Addresses, Buildings, Parcels with tax/assessment data\n",
    "3. **Geodatabase** - Spatial layers (requires GIS tools)\n",
    "\n",
    "**Primary Goal**: Hunt for blight/inhabitability labels for machine learning classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä Detroit Blight Classification EDA - Ready to analyze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Blight Survey Data Analysis - HUNTING FOR LABELS üéØ\n",
    "\n",
    "This is our most promising dataset for blight classification labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_blight_survey_data():\n",
    "    \"\"\"\n",
    "    Analyze the Detroit Land Bank Authority blight survey data\n",
    "    Focus on finding classification labels for blight/inhabitability\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üè† BLIGHT SURVEY DATA ANALYSIS - LABEL HUNTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load the Excel file - Updated path for new location\n",
    "    file_path = \"../../data/blight_survey_data/20250527_DLBA_survey_data_UM_Detroit.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        print(f\"üìã Dataset Shape: {df.shape}\")\n",
    "        print(f\"üìä Number of rows: {df.shape[0]:,}\")\n",
    "        print(f\"üìã Number of columns: {df.shape[1]}\")\n",
    "        print()\n",
    "        \n",
    "        # Focus on potential LABEL columns\n",
    "        label_candidates = [\n",
    "            'FIELD_DETERMINATION', 'FINAL_DETERMINATION', 'FINAL_DETERMINATION_2',\n",
    "            'OTHER_RESOLUTION_PATHWAYS_DETERMINATION', 'SURVEY_STATUS',\n",
    "            'HAS_STRUCTURE', 'IS_OCCUPIED'\n",
    "        ]\n",
    "        \n",
    "        print(\"üéØ POTENTIAL BLIGHT CLASSIFICATION LABELS:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for col in label_candidates:\n",
    "            if col in df.columns:\n",
    "                null_count = df[col].isnull().sum()\n",
    "                null_pct = (null_count / len(df)) * 100\n",
    "                unique_vals = df[col].nunique()\n",
    "                \n",
    "                print(f\"\\nüè∑Ô∏è  {col}:\")\n",
    "                print(f\"   Unique values: {unique_vals}\")\n",
    "                print(f\"   Missing: {null_count:,} ({null_pct:.1f}%)\")\n",
    "                \n",
    "                if unique_vals <= 20 and unique_vals > 0:\n",
    "                    print(f\"   Values: {sorted(df[col].dropna().unique())}\")\n",
    "                    print(f\"   Value counts:\")\n",
    "                    value_counts = df[col].value_counts(dropna=False)\n",
    "                    for val, count in value_counts.head(10).items():\n",
    "                        pct = (count / len(df)) * 100\n",
    "                        print(f\"     {val}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Look at condition assessment columns\n",
    "        condition_cols = [\n",
    "            'FACADE_SIDING_CONDITION', 'FIRE_DAMAGE_CONDITION', 'ROOF_CONDITION',\n",
    "            'OPENINGS_CONDITION', 'IS_OPEN_TO_TRESPASS', 'PORCH_STEPS_CONDITION'\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n\\nüèöÔ∏è PROPERTY CONDITION FEATURES (for ML features):\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for col in condition_cols:\n",
    "            if col in df.columns:\n",
    "                null_count = df[col].isnull().sum()\n",
    "                null_pct = (null_count / len(df)) * 100\n",
    "                unique_vals = df[col].nunique()\n",
    "                \n",
    "                print(f\"\\nüîß {col}:\")\n",
    "                print(f\"   Unique values: {unique_vals}\")\n",
    "                print(f\"   Missing: {null_count:,} ({null_pct:.1f}%)\")\n",
    "                \n",
    "                if unique_vals <= 15 and unique_vals > 0:\n",
    "                    value_counts = df[col].value_counts(dropna=False)\n",
    "                    for val, count in value_counts.head(5).items():\n",
    "                        pct = (count / len(df)) * 100\n",
    "                        print(f\"     {val}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        print(\"\\nüìù Column Information (All columns):\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            dtype = str(df[col].dtype)\n",
    "            null_count = df[col].isnull().sum()\n",
    "            null_pct = (null_count / len(df)) * 100\n",
    "            print(f\"{i:2d}. {col:<35} | {dtype:<15} | {null_count:>5} nulls ({null_pct:5.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading blight survey data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "blight_df = analyze_blight_survey_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. COD Layers Analysis - More Label Hunting üîç\n",
    "\n",
    "Looking for blight indicators in the City of Detroit address, building, and parcel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cod_layers_csv():\n",
    "    \"\"\"\n",
    "    Analyze the City of Detroit (COD) layers CSV data\n",
    "    Hunt for blight/inhabitability indicators\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèôÔ∏è COD LAYERS CSV DATA ANALYSIS - BLIGHT HUNTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Updated path for new location\n",
    "    data_dir = \"../../data/cod_layers_csv/20250728_CODLayers.csv\"\n",
    "    csv_files = [\"Addresses.csv\", \"Buildings.csv\", \"Parcels2025.csv\"]\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(data_dir, csv_file)\n",
    "        dataset_name = csv_file.replace('.csv', '')\n",
    "        \n",
    "        print(f\"\\nüè¢ {dataset_name.upper()} Dataset:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "            datasets[dataset_name] = df\n",
    "            \n",
    "            print(f\"üìä Dataset Shape: {df.shape}\")\n",
    "            print(f\"üìã Rows: {df.shape[0]:,}, Columns: {df.shape[1]}\")\n",
    "            \n",
    "            # Hunt for blight-related columns\n",
    "            blight_keywords = ['status', 'condition', 'class', 'vacant', 'occupied', 'tax', 'value', 'assessment']\n",
    "            \n",
    "            potential_blight_cols = []\n",
    "            for col in df.columns:\n",
    "                for keyword in blight_keywords:\n",
    "                    if keyword.lower() in col.lower():\n",
    "                        potential_blight_cols.append(col)\n",
    "                        break\n",
    "            \n",
    "            if potential_blight_cols:\n",
    "                print(f\"\\nüéØ POTENTIAL BLIGHT INDICATORS ({len(potential_blight_cols)} found):\")\n",
    "                for col in potential_blight_cols:\n",
    "                    unique_vals = df[col].nunique()\n",
    "                    null_count = df[col].isnull().sum()\n",
    "                    null_pct = (null_count / len(df)) * 100\n",
    "                    \n",
    "                    print(f\"\\n   üè∑Ô∏è {col}:\")\n",
    "                    print(f\"      Unique values: {unique_vals}, Missing: {null_pct:.1f}%\")\n",
    "                    \n",
    "                    if unique_vals <= 20 and unique_vals > 0:\n",
    "                        value_counts = df[col].value_counts(dropna=False)\n",
    "                        for val, count in value_counts.head(5).items():\n",
    "                            pct = (count / len(df)) * 100\n",
    "                            print(f\"        {val}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Show all columns for reference\n",
    "            print(f\"\\nüìù All Columns ({df.shape[1]}):\")    \n",
    "            for i, col in enumerate(df.columns, 1):\n",
    "                dtype = str(df[col].dtype)\n",
    "                null_count = df[col].isnull().sum()\n",
    "                null_pct = (null_count / len(df)) * 100\n",
    "                blight_flag = \"üéØ\" if col in potential_blight_cols else \"  \"\n",
    "                print(f\"{blight_flag}{i:2d}. {col:<30} | {dtype:<15} | {null_count:>6} nulls ({null_pct:5.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {csv_file}: {e}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Run the analysis\n",
    "cod_datasets = analyze_cod_layers_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Dive into Parcels Data - Tax and Assessment Indicators üí∞\n",
    "\n",
    "The Parcels2025 dataset likely has the richest blight indicators through tax status, property values, and conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into Parcels data for blight indicators\n",
    "if 'Parcels2025' in cod_datasets:\n",
    "    parcels_df = cod_datasets['Parcels2025']\n",
    "    \n",
    "    print(\"üè° PARCELS 2025 - DEEP DIVE FOR BLIGHT LABELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Key columns that might indicate blight\n",
    "    key_blight_cols = [\n",
    "        'Tax Status', 'Tax Status Description', 'Property Class', 'Property Class Description',\n",
    "        'Use Code', 'Use Code Description', 'Assessed Value', 'Taxable Value', \n",
    "        'Sale Price', 'Is Improved', 'Year Built', 'Building Style', 'Building Count'\n",
    "    ]\n",
    "    \n",
    "    for col in key_blight_cols:\n",
    "        if col in parcels_df.columns:\n",
    "            print(f\"\\nüîç {col}:\")\n",
    "            \n",
    "            unique_vals = parcels_df[col].nunique()\n",
    "            null_count = parcels_df[col].isnull().sum()\n",
    "            null_pct = (null_count / len(parcels_df)) * 100\n",
    "            \n",
    "            print(f\"   Unique: {unique_vals}, Missing: {null_pct:.1f}%\")\n",
    "            \n",
    "            if unique_vals <= 50 and unique_vals > 0:\n",
    "                print(\"   Top values:\")\n",
    "                value_counts = parcels_df[col].value_counts(dropna=False)\n",
    "                for val, count in value_counts.head(10).items():\n",
    "                    pct = (count / len(parcels_df)) * 100\n",
    "                    print(f\"     {val}: {count:,} ({pct:.1f}%)\")\n",
    "            elif parcels_df[col].dtype in ['int64', 'float64']:\n",
    "                print(f\"   Stats: Mean={parcels_df[col].mean():.2f}, Median={parcels_df[col].median():.2f}\")\n",
    "                print(f\"   Range: {parcels_df[col].min():.2f} to {parcels_df[col].max():.2f}\")\n",
    "    \n",
    "    # Look for zero/low value properties (potential blight indicator)\n",
    "    print(\"\\nüí∞ LOW VALUE PROPERTIES (Potential Blight Indicators):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Zero assessed value\n",
    "    zero_assessed = (parcels_df['Assessed Value'] == 0).sum()\n",
    "    zero_assessed_pct = (zero_assessed / len(parcels_df)) * 100\n",
    "    print(f\"Properties with $0 Assessed Value: {zero_assessed:,} ({zero_assessed_pct:.1f}%)\")\n",
    "    \n",
    "    # Very low assessed value (under $1000)\n",
    "    low_assessed = (parcels_df['Assessed Value'] < 1000).sum()\n",
    "    low_assessed_pct = (low_assessed / len(parcels_df)) * 100\n",
    "    print(f\"Properties with <$1,000 Assessed Value: {low_assessed:,} ({low_assessed_pct:.1f}%)\")\n",
    "    \n",
    "    # Zero sale price\n",
    "    zero_sale = (parcels_df['Sale Price'] == 0).sum()\n",
    "    zero_sale_pct = (zero_sale / len(parcels_df)) * 100\n",
    "    print(f\"Properties with $0 Sale Price: {zero_sale:,} ({zero_sale_pct:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Parcels2025 data not available for deep dive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Label Creation Strategy üéØ\n",
    "\n",
    "Based on our analysis, let's create potential blight classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blight_labels(blight_df, parcels_df):\n",
    "    \"\"\"\n",
    "    Create potential blight classification labels from the datasets\n",
    "    \"\"\"\n",
    "    print(\"üéØ CREATING BLIGHT CLASSIFICATION LABELS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    labels_summary = {}\n",
    "    \n",
    "    if blight_df is not None:\n",
    "        print(\"\\nüè† BLIGHT SURVEY LABELS:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Option 1: Final Determination as primary label\n",
    "        if 'FINAL_DETERMINATION' in blight_df.columns:\n",
    "            valid_determinations = blight_df['FINAL_DETERMINATION'].dropna()\n",
    "            labels_summary['final_determination'] = {\n",
    "                'count': len(valid_determinations),\n",
    "                'unique_values': valid_determinations.nunique(),\n",
    "                'distribution': valid_determinations.value_counts().to_dict()\n",
    "            }\n",
    "            print(f\"‚úÖ FINAL_DETERMINATION: {len(valid_determinations):,} valid labels\")\n",
    "            print(f\"   Categories: {valid_determinations.nunique()}\")\n",
    "            \n",
    "        # Option 2: Field Determination as backup\n",
    "        if 'FIELD_DETERMINATION' in blight_df.columns:\n",
    "            valid_field = blight_df['FIELD_DETERMINATION'].dropna()\n",
    "            labels_summary['field_determination'] = {\n",
    "                'count': len(valid_field),\n",
    "                'unique_values': valid_field.nunique(),\n",
    "                'distribution': valid_field.value_counts().to_dict()\n",
    "            }\n",
    "            print(f\"‚úÖ FIELD_DETERMINATION: {len(valid_field):,} valid labels\")\n",
    "            print(f\"   Categories: {valid_field.nunique()}\")\n",
    "    \n",
    "    if 'Parcels2025' in cod_datasets:\n",
    "        print(\"\\nüè° PARCELS-BASED LABELS:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Create binary blight indicator based on low property values\n",
    "        low_value_threshold = 1000\n",
    "        blight_indicator = (parcels_df['Assessed Value'] < low_value_threshold) & (parcels_df['Assessed Value'] > 0)\n",
    "        \n",
    "        blight_count = blight_indicator.sum()\n",
    "        blight_pct = (blight_count / len(parcels_df)) * 100\n",
    "        \n",
    "        labels_summary['low_value_blight'] = {\n",
    "            'count': len(parcels_df),\n",
    "            'blight_properties': blight_count,\n",
    "            'blight_percentage': blight_pct\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ LOW VALUE BLIGHT (< ${low_value_threshold}): {blight_count:,} properties ({blight_pct:.1f}%)\")\n",
    "        \n",
    "        # Tax status based labels\n",
    "        if 'Tax Status Description' in parcels_df.columns:\n",
    "            tax_status_counts = parcels_df['Tax Status Description'].value_counts()\n",
    "            labels_summary['tax_status'] = tax_status_counts.to_dict()\n",
    "            print(f\"‚úÖ TAX STATUS LABELS: {len(tax_status_counts)} categories\")\n",
    "            for status, count in tax_status_counts.head(5).items():\n",
    "                pct = (count / len(parcels_df)) * 100\n",
    "                print(f\"   {status}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüéØ RECOMMENDED LABELING STRATEGY:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"1. PRIMARY: Use FINAL_DETERMINATION from blight survey (highest quality)\")\n",
    "    print(\"2. SECONDARY: Use FIELD_DETERMINATION as backup/additional data\")\n",
    "    print(\"3. FEATURES: Combine with property conditions, tax status, assessed values\")\n",
    "    print(\"4. AUGMENT: Create synthetic labels using low assessed values\")\n",
    "    \n",
    "    return labels_summary\n",
    "\n",
    "# Create the labels\n",
    "if blight_df is not None and 'Parcels2025' in cod_datasets:\n",
    "    label_analysis = create_blight_labels(blight_df, cod_datasets['Parcels2025'])\n",
    "else:\n",
    "    print(\"‚ùå Cannot create labels - missing required datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Geodatabase Information üó∫Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_geodatabase_info():\n",
    "    \"\"\"\n",
    "    Analyze the geodatabase structure (limited without specialized GIS tools)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üó∫Ô∏è GEODATABASE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Updated path for new location\n",
    "    gdb_path = \"../../data/cod_layers_gdb/CODBaseUnitLayers.gdb\"\n",
    "    \n",
    "    if os.path.exists(gdb_path):\n",
    "        print(f\"üìç Geodatabase found at: {gdb_path}\")\n",
    "        \n",
    "        # Get file information\n",
    "        files = os.listdir(gdb_path)\n",
    "        print(f\"üìÅ Number of files in geodatabase: {len(files)}\")\n",
    "        \n",
    "        # Categorize files by extension\n",
    "        extensions = {}\n",
    "        for file in files:\n",
    "            ext = os.path.splitext(file)[1] if '.' in file else 'no_extension'\n",
    "            extensions[ext] = extensions.get(ext, 0) + 1\n",
    "        \n",
    "        print(\"\\nüìã File types in geodatabase:\")\n",
    "        for ext, count in sorted(extensions.items()):\n",
    "            print(f\"  {ext}: {count} files\")\n",
    "        \n",
    "        # Get total size\n",
    "        total_size = sum(os.path.getsize(os.path.join(gdb_path, f)) for f in files)\n",
    "        print(f\"\\nüíæ Total geodatabase size: {total_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è  Note: This is an ESRI geodatabase (.gdb) which requires\")\n",
    "        print(\"   specialized GIS software (like ArcGIS or QGIS with GDAL) for full analysis.\")\n",
    "        print(\"   Consider using geopandas or arcpy for detailed geodatabase exploration.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Geodatabase not found!\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Analyze geodatabase\n",
    "gdb_info = analyze_geodatabase_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps üìã\n",
    "\n",
    "Key findings for Project 2 blight classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä PROJECT 2 BLIGHT CLASSIFICATION - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if blight_df is not None:\n",
    "    print(f\"‚úÖ Blight Survey Data: {blight_df.shape[0]:,} rows, {blight_df.shape[1]} columns\")\n",
    "    \n",
    "    # Check key label columns\n",
    "    final_det_count = blight_df['FINAL_DETERMINATION'].notna().sum() if 'FINAL_DETERMINATION' in blight_df.columns else 0\n",
    "    field_det_count = blight_df['FIELD_DETERMINATION'].notna().sum() if 'FIELD_DETERMINATION' in blight_df.columns else 0\n",
    "    \n",
    "    print(f\"   üéØ FINAL_DETERMINATION labels: {final_det_count:,}\")\n",
    "    print(f\"   üéØ FIELD_DETERMINATION labels: {field_det_count:,}\")\n",
    "\n",
    "if cod_datasets:\n",
    "    for name, df in cod_datasets.items():\n",
    "        print(f\"‚úÖ COD {name}: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "\n",
    "print(\"‚úÖ Geodatabase: 284.45 MB spatial data\")\n",
    "\n",
    "print(\"\\nüéØ BLIGHT CLASSIFICATION RECOMMENDATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. **PRIMARY LABELS**: Use FINAL_DETERMINATION from blight survey\")\n",
    "print(\"2. **BACKUP LABELS**: Use FIELD_DETERMINATION for additional training data\")\n",
    "print(\"3. **FEATURES**: Property conditions, tax status, assessed values, building age\")\n",
    "print(\"4. **AUGMENTATION**: Create binary labels from low property values (<$1000)\")\n",
    "print(\"5. **SPATIAL**: Use geodatabase for neighborhood context features\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"- Clean and preprocess the blight survey labels\")\n",
    "print(\"- Join datasets on parcel_id for feature engineering\")\n",
    "print(\"- Create train/test splits preserving spatial distribution\")\n",
    "print(\"- Build baseline models (Random Forest, XGBoost)\")\n",
    "print(\"- Explore advanced models with spatial features\")\n",
    "\n",
    "print(\"\\nüìä Dataset Ready for Machine Learning! ü§ñ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blight_df['FINAL_DETERMINATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midas_aug25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
